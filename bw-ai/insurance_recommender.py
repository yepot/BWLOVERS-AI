import json
import os
import uuid
import re
from typing import Dict, Any, List
from datetime import datetime

# FAISS ê¸°ë°˜ RAG + LLM
try:
    from langchain_community.vectorstores import FAISS
    from langchain_huggingface import HuggingFaceEmbeddings
    from langchain_core.documents import Document
    
    # rag_pipeline import (LLM + í”„ë¡¬í”„íŠ¸)
    from rag_pipeline import ask_question
    
    # ìž„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
    embeddings = HuggingFaceEmbeddings(
        model_name="jhgan/ko-sroberta-multitask",
        model_kwargs={"device": "cpu"},
        encode_kwargs={"normalize_embeddings": True},
    )
    
    # FAISS ë²¡í„°ìŠ¤í† ì–´ ê²½ë¡œ
    faiss_path = "../faiss_index"
    
    # FAISS ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ë˜ëŠ” ìƒì„±
    if os.path.exists(f"{faiss_path}/index.faiss"):
        vectorstore = FAISS.load_local(faiss_path, embeddings, allow_dangerous_deserialization=True)
        print(f"âœ… ê¸°ì¡´ FAISS ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œë¨: {vectorstore.index.ntotal}ê°œ ë¬¸ì„œ")
    else:
        vectorstore = None
        print("ðŸ†• FAISS ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì˜ˆì •")
    
    RAG_AVAILABLE = True
    LLM_AVAILABLE = True
    print("âœ… FAISS + LLM ê¸°ë°˜ RAG ì‹œìŠ¤í…œ í™œì„±í™”ë¨")
    
except ImportError as e:
    print(f"RAG ì‹œìŠ¤í…œ ìž„í¬íŠ¸ ì‹¤íŒ¨: {e}")
    vectorstore = None
    RAG_AVAILABLE = False
    LLM_AVAILABLE = False
    
except Exception as e:
    print(f"RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    vectorstore = None
    RAG_AVAILABLE = False
    LLM_AVAILABLE = False

# íŒŒì¼ ìƒë‹¨ì— ìœ„ì¹˜
PRICE_MAP = {}
# __file__ì˜ ì ˆëŒ€ ê²½ë¡œë¥¼ ê°€ì ¸ì™€ì„œ ê²½ë¡œ ë¬¸ì œ ë°©ì§€
PRICE_FILE = os.path.join(os.path.dirname(os.path.abspath(__file__)), "prices.json")

SUM_INSURED_MAP = {}
SUM_INSURED_FILE = os.path.join(os.path.dirname(os.path.abspath(__file__)), "sum_insured.json")

def _load_price_map():
    """ë³´í—˜ë£Œ í…Œì´ë¸” ë¡œë“œ"""
    global PRICE_MAP
    if not os.path.exists(PRICE_FILE):
        print(f"ë³´í—˜ë£Œ íŒŒì¼ ì—†ìŒ: {PRICE_FILE} (ê¸°ë³¸ê°’ ì‚¬ìš©)")
        return

    try:
        with open(PRICE_FILE, "r", encoding="utf-8") as f:
            PRICE_MAP = json.load(f)
        print(f"âœ… ë³´í—˜ë£Œ í…Œì´ë¸” ë¡œë“œë¨: {len(PRICE_MAP)}ê°œ ë³´í—˜ì‚¬")
    except json.JSONDecodeError:
        print(f"ë³´í—˜ë£Œ íŒŒì¼ í˜•ì‹ì´ ìž˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤ (JSON íŒŒì‹± ì‹¤íŒ¨)")
    except Exception as e:
        print(f"ë³´í—˜ë£Œ í…Œì´ë¸” ë¡œë“œ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")


def _load_sum_insured_map():
    """ë³´í—˜ê°€ìž…ê¸ˆì•¡ í…Œì´ë¸” ë¡œë“œ"""
    global SUM_INSURED_MAP
    if not os.path.exists(SUM_INSURED_FILE):
        print(f"ê°€ìž…ê¸ˆì•¡ íŒŒì¼ ì—†ìŒ: {SUM_INSURED_FILE} (ê¸°ë³¸ê°’ ì‚¬ìš©)")
        return

    try:
        with open(SUM_INSURED_FILE, "r", encoding="utf-8") as f:
            SUM_INSURED_MAP = json.load(f)
        print(f"âœ… ê°€ìž…ê¸ˆì•¡ í…Œì´ë¸” ë¡œë“œë¨: {len(SUM_INSURED_MAP)}ê°œ ë³´í—˜ì‚¬")
    except json.JSONDecodeError:
        print("ê°€ìž…ê¸ˆì•¡ íŒŒì¼ í˜•ì‹ì´ ìž˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤ (JSON íŒŒì‹± ì‹¤íŒ¨)")
    except Exception as e:
        print(f"ê°€ìž…ê¸ˆì•¡ í…Œì´ë¸” ë¡œë“œ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")

# ì‹¤í–‰ë¶€
_load_price_map()
_load_price_map()
_load_sum_insured_map()

class InsuranceRecommender:
    """
    FAISS + LLM ê¸°ë°˜ ì§„ì§œ RAG ë³´í—˜ ì¶”ì²œ ì‹œìŠ¤í…œ
    """
    
    def __init__(self):
        if RAG_AVAILABLE:
            self.vectorstore = vectorstore
            self.embeddings = embeddings
            self._load_insurance_data()
        else:
            print("RAG ì—†ì´ ê¸°ë³¸ ì¶”ì²œ ëª¨ë“œë¡œ ìž‘ë™")
    
    def _load_insurance_data(self):
        """ëª¨ë“  JSON ë°ì´í„°ë¥¼ FAISSì— ë¡œë“œ"""
        try:
            if self.vectorstore and hasattr(self.vectorstore, 'index') and self.vectorstore.index.ntotal > 0:
                print(f"âœ… FAISSì— ì´ë¯¸ {self.vectorstore.index.ntotal}ê°œ ë¬¸ì„œ ì¡´ìž¬")
                return
            
            documents = []
            data_dir = "../json/Llama_json"
            
            if os.path.exists(data_dir):
                json_files = [f for f in os.listdir(data_dir) if f.endswith('.json')]
                for filename in json_files:
                    filepath = os.path.join(data_dir, filename)
                    try:
                        with open(filepath, 'r', encoding='utf-8') as f:
                            data = json.load(f)
                        
                        if isinstance(data, list):
                            for item in data:
                                content = item.get('content', '').strip()
                                metadata = item.get('metadata', {})
                                if content and len(content) > 20:
                                    doc = Document(
                                        page_content=content,
                                        metadata={**metadata, 'source_file': filename, 'chunk_type': 'full_content'}
                                    )
                                    documents.append(doc)
                    except Exception as e:
                        print(f" {filename} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            
            if documents:
                self.vectorstore = FAISS.from_documents(documents, self.embeddings)
                self.vectorstore.save_local(faiss_path)
                print(f"âœ… FAISS ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ: {len(documents)}ê°œ ë¬¸ì„œ")
        except Exception as e:
            print(f"ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")

    def search_relevant_documents(self, query: str, n_results: int = 10) -> List[Document]:
        if not RAG_AVAILABLE or not self.vectorstore:
            return []
        try:
            docs_with_scores = self.vectorstore.similarity_search_with_score(query, k=n_results)
            return [doc for doc, score in docs_with_scores]
        except Exception as e:
            print(f"FAISS ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

    def generate_rag_recommendation(self, user_profile: Dict[str, Any], health_status: Dict[str, Any]) -> Dict[str, Any]:
        if not LLM_AVAILABLE:
            return self._fallback_recommendation(user_profile, health_status)
        try:
            analysis = self._analyze_user_profile(user_profile, health_status)
            search_query = self._build_rag_query(analysis)
            relevant_docs = self.search_relevant_documents(search_query, n_results=15)
            
            if not relevant_docs:
                return self._fallback_recommendation(user_profile, health_status)
            
            context = self._build_context_from_documents(relevant_docs)
            llm_question = self._build_llm_question(analysis, context)
            rag_result = ask_question(llm_question, profile=analysis)
            
            if rag_result and 'answer' in rag_result:
                structured_recommendation = self._parse_llm_response_to_recommendation(
                    rag_result['answer'], analysis, relevant_docs
                )
                self._log_rag_performance(rag_result, analysis, relevant_docs)
                return structured_recommendation
            return self._fallback_recommendation(user_profile, health_status)
        except Exception as e:
            print(f"RAG ì¶”ì²œ ìƒì„± ì‹¤íŒ¨: {e}")
            return self._fallback_recommendation(user_profile, health_status)

    def _build_rag_query(self, analysis: Dict[str, Any]) -> str:
        parts = ["ìž„ì‹  ë³´í—˜", f"{analysis.get('gestational_week', 0)}ì£¼ì°¨"]

        if analysis.get("is_multiple_pregnancy"):
            parts.append("ë‹¤íƒœì•„")
        
        if (analysis.get("miscarriage_history") or 0) > 0:
            parts.append("ìœ ì‚°ë ¥")
        
        if analysis.get("has_preeclampsia"):
            parts.extend(["ìž„ì‹ ì¤‘ë…ì¦", "ê³ í˜ˆì••", "ì§„ë‹¨ë¹„"])
        
        if analysis.get("has_preterm_risk"):
            parts.extend(["ì¡°ì‚°", "ë¯¸ìˆ™ì•„", "NICU", "ìž…ì›"])
        
        if analysis.get("has_diabetes"):
            parts.extend(["ë‹¹ë‡¨", "í•©ë³‘ì¦"])

        rf = analysis.get("risk_factors") or []
        parts.extend(rf)
        return " ".join(parts)

    def _build_context_from_documents(self, documents: List[Document]) -> str:
        context_parts = []
        for i, doc in enumerate(documents[:10]):
            md = doc.metadata or {}
            context_part = (
                f"[ë¬¸ì„œ {i+1}]\n"
                f"product_name: {md.get('product_name', 'ì•Œ ìˆ˜ ì—†ìŒ')}\n"
                f"page_number: {md.get('page_number', md.get('page', 'N/A'))}\n"
                f"source_file: {md.get('source_file', md.get('source', 'N/A'))}\n"
                f"content: {doc.page_content[:1000]}\n"
                f"---"
            )
            context_parts.append(context_part)
        return "\n".join(context_parts)

    def _build_llm_question(self, analysis: Dict[str, Any], context: str) -> str:
        return f"""
[ì ˆëŒ€ ì¤€ìˆ˜ ì‚¬í•­]
- ë‹µë³€ì€ ë°˜ë“œì‹œ ìœ íš¨í•œ JSON í¬ë§·ì´ì–´ì•¼ í•œë‹¤.
- ëª¨ë“  ë¬¸ìžì—´ì€ í‘œì¤€ í°ë”°ì˜´í‘œ(")ë¡œë§Œ ê°ì‹¸ì•¼ í•œë‹¤.
- í•œêµ­ì–´ íŠ¹ìˆ˜ ë”°ì˜´í‘œ(ã€Œ, ã€, ã€Ž, ã€)ëŠ” ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆë¼. ì¸ìš© ì‹œì—ë„ í‘œì¤€ í°ë”°ì˜´í‘œ(")ë¥¼ ì‚¬ìš©í•˜ë¼.

[ì—­í• ]
ë„ˆëŠ” ë³´í—˜ ì•½ê´€ ì „ë¬¸ ë¶„ì„ê°€ë‹¤. ì œê³µëœ ë¬¸ë§¥(context)ë§Œ ê·¼ê±°ë¡œ ë‹µí•´ì•¼ í•œë‹¤. ë°˜ë“œì‹œ JSONë§Œ ì¶œë ¥í•˜ë¼. (ë‹¤ë¥¸ ë¬¸ìž¥, ì„¤ëª…, ì½”ë“œë¸”ë¡ ê¸ˆì§€)
ëª¨ë“  ê°’ì€ í°ë”°ì˜´í‘œ " ë§Œ ì‚¬ìš©í•œë‹¤. (ìž‘ì€ë”°ì˜´í‘œ/ã€Œã€ ê¸ˆì§€)

[í•µì‹¬ ì›ì¹™]
- ë¬¸ë§¥ì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ê³  â€œë¬¸ë§¥ì— ì—†ìŒâ€ìœ¼ë¡œ ëª…ì‹œí•œë‹¤.
- ê·¼ê±°ëŠ” ë°˜ë“œì‹œ ë¬¸ë§¥ì—ì„œ 1~2ë¬¸ìž¥ ê·¸ëŒ€ë¡œ ì¸ìš©í•˜ë¼.
- ê·¼ê±°ì—ëŠ” ë°˜ë“œì‹œ íŽ˜ì´ì§€ ë²ˆí˜¸ë¥¼ í¬í•¨í•˜ë¼. (contextì— page_numberê°€ ìžˆë‹¤)

[ì •í™•ë„ ê°•í™” ê·œì¹™]
- ê²°ë¡ ì—ëŠ” ë°˜ë“œì‹œ ì§ˆë¬¸ í‚¤ì›Œë“œ(ì˜ˆ: ìž„ì‹ /ë‹¹ë‡¨/ì¡°ì‚°) + ë³´ìž¥í•­ëª©/íŠ¹ì•½ëª…ì„ í•¨ê»˜ í¬í•¨.
- ê·¼ê±°ëŠ” ìµœì†Œ 2ê°œ ì œì‹œ(ê°€ëŠ¥í•˜ë©´ ì„œë¡œ ë‹¤ë¥¸ ë¬¸ìž¥).
- ê´€ë ¨ ì¡°í•­ì´ ì—†ìœ¼ë©´ â€œë¬¸ë§¥ì— ì—†ìŒâ€ìœ¼ë¡œ ëª…ì‹œí•˜ë˜, ìœ ì‚¬/ìƒìœ„ ë²”ì£¼ëŠ” ì œì‹œí•˜ë¼.

[ìž„ì‹ ë¶€ ì •ë³´]
- ìž„ì‹  ì£¼ìˆ˜: {analysis.get('gestational_week', 0)}ì£¼ì°¨
- ìœ„í—˜ìš”ì¸: {analysis.get('risk_factors') or []}
- ë‹¤íƒœì•„: {analysis.get('is_multiple_pregnancy', False)}
- ìœ ì‚°ë ¥: {analysis.get('miscarriage_history', 0)}

[ë³´í—˜ ì•½ê´€ ì •ë³´]
{context}

[ì¶œë ¥ ê·œì¹™]
- ë°˜ë“œì‹œ JSONë§Œ ì¶œë ¥ (ì„¤ëª… ë¬¸ìž¥/ë§ˆí¬ë‹¤ìš´/ì½”ë“œë¸”ë¡ ê¸ˆì§€)
- evidenceëŠ” ë¬¸ë§¥ì—ì„œ â€œê·¸ëŒ€ë¡œ ì¸ìš©í•œ ë¬¸ìž¥â€ë§Œ í—ˆìš©
- evidence ì•ˆì— ë°˜ë“œì‹œ (page=ìˆ«ìž) í¬í•¨
- special_contractsëŠ” ë¬¸ìžì—´ ë°°ì—´(1~3ê°œ)
- monthly_costëŠ” ì •ìˆ˜ (ë¬¸ë§¥ì— ì—†ìœ¼ë©´ í•©ë¦¬ì  ì¶”ì •, ì¶”ì •ìž„ì„ reasonì— ëª…ì‹œ)

{{
  "recommendations": [
    {{
      "company": "ë³´í—˜ì‚¬ëª…(ë¬¸ë§¥ì— ì—†ìœ¼ë©´ 'ì•Œ ìˆ˜ ì—†ìŒ')",
      "product": "ìƒí’ˆëª…(ë¬¸ë§¥ì— ì—†ìœ¼ë©´ 'ì•Œ ìˆ˜ ì—†ìŒ')",
      "monthly_cost": 10000,
      "reason": "ì¶”ì²œ ì´ìœ (í‚¤ì›Œë“œ+ë³´ìž¥/íŠ¹ì•½ ì—°ê²°, ë¬¸ë§¥ ê·¼ê±° ì–¸ê¸‰)",
      "special_contracts": ["íŠ¹ì•½1", "íŠ¹ì•½2"],
      "evidence": "ë¬¸ë§¥ ì¸ìš©ë¬¸... (page=ìˆ«ìž)"
    }}
  ]
}}
""".strip()

    def _parse_llm_response_to_recommendation(
        self,
        llm_response: str,
        analysis: Dict[str, Any],
        relevant_docs: List[Document],
    ) -> Dict[str, Any]:
        try:
            json_block = re.search(r"(\{.*\})", llm_response, re.DOTALL)
            if not json_block:
                return self._fallback_recommendation(analysis, {})

            raw = json_block.group(1)
            fixed = self._fix_json_string(raw)

            try:
                llm_json = json.loads(fixed)
            except Exception:
                import ast
                fixed2 = fixed.replace("true", "True").replace("false", "False").replace("null", "None")
                llm_json = ast.literal_eval(fixed2)

            recs = llm_json.get("recommendations", [])
            if not isinstance(recs, list):
                return self._fallback_recommendation(analysis, {})

            items: List[Dict[str, Any]] = []
            for idx, rec in enumerate(recs[:3]):
                if not isinstance(rec, dict):
                    continue

                doc = relevant_docs[idx] if idx < len(relevant_docs) else (relevant_docs[0] if relevant_docs else None)
                md = (doc.metadata or {}) if doc else {}
                doc_page = md.get("page_number") or md.get("page") or 1
                doc_product = md.get("product_name") or "ì•Œ ìˆ˜ ì—†ìŒ"

                company = rec.get("company", "ì•Œ ìˆ˜ ì—†ìŒ")
                product = rec.get("product", "ì¶”ì²œ ìƒí’ˆ")
                llm_cost = rec.get("monthly_cost", 0) or 0
                sum_insured = self._get_sum_insured(company, product)
                monthly_cost = self._get_insurance_price(company, product)
                if monthly_cost == 10000 and llm_cost > 0:
                    monthly_cost = llm_cost
                reason = rec.get("reason", "") or ""

                if company in ("ë³´í—˜ì‚¬ëª…", "ì•Œ ìˆ˜ ì—†ìŒ", "", None):
                    company = self._extract_company_from_metadata(md) if doc else "ì•Œ ìˆ˜ ì—†ìŒ"
                if product in ("ìƒí’ˆëª…", "ì¶”ì²œ ìƒí’ˆ", "ì•Œ ìˆ˜ ì—†ìŒ", "", None):
                    product = doc_product

                contracts = rec.get("special_contracts", []) or []
                if not isinstance(contracts, list):
                    contracts = []

                special_contracts_out: List[Dict[str, Any]] = []
                for c in contracts[:3]:
                    name = c if isinstance(c, str) else str(c)
                    special_contracts_out.append({
                        "contract_name": name,
                        "contract_description": f"{name}ì— ëŒ€í•œ ì•½ê´€ ê¸°ë°˜ ë³´ìž¥/ì¡°ê±´ ìš”ì•½",
                        "contract_recommendation_reason": (
                            f"{analysis.get('gestational_week', 0)}ì£¼ì°¨ ë° "
                            f"ìœ„í—˜ìš”ì¸({', '.join(analysis.get('risk_factors', [])) or 'ì—†ìŒ'}) ê¸°ì¤€ ì¶”ì²œ"
                        ),
                        "key_features": [
                            "ì•½ê´€ ê·¼ê±°ë¡œ ë³´ìž¥ ë²”ìœ„/ì¡°ê±´ í™•ì¸",
                            "ìž„ì‹  ì£¼ìˆ˜ ë° ê±´ê°•ìƒíƒœ ê¸°ë°˜ í•„ìš” ë³´ìž¥ ìš°ì„ ìˆœìœ„ ë°˜ì˜",
                        ],
                        "page_number": int(doc_page),
                    })

                evidence = rec.get("evidence", "") or ""
                if not evidence and doc:
                    evidence = doc.page_content[:200]

                evidence_sources_out = [{
                    "page_number": int(doc_page),
                    "text_snippet": str(evidence)[:500],
                }]

                items.append({
                    "itemId": f"rag-{uuid.uuid4().hex[:8]}",
                    "insurance_company": company,
                    "product_name": product,
                    "is_long_term": True,
                    "sum_insured": int(sum_insured),
                    "monthly_cost": str(monthly_cost),
                    "insurance_recommendation_reason": reason,
                    "special_contracts": special_contracts_out,
                    "evidence_sources": evidence_sources_out,
                })

            return {
                "resultId": f"rag-{uuid.uuid4().hex[:8]}",
                "expiresInSec": 600,
                "items": items,
                "rag_metadata": {
                    "llm_response_quality": self._evaluate_response_quality(llm_response, analysis),
                    "documents_used": len(relevant_docs),
                    "gestational_week": analysis.get("gestational_week", 0),
                    "risk_factors": analysis.get("risk_factors", []),
                },
            }

        except Exception as e:
            print(f"íŒŒì‹±/êµ¬ì¡°í™” ì‹¤íŒ¨: {e}")
            return self._fallback_recommendation(analysis, {})
        

    
    def _fix_json_string(self, json_str: str) -> str:
        if not json_str:
            return ""
        
        # 1. í•œêµ­ì–´ íŠ¹ìˆ˜ ë”°ì˜´í‘œë¥¼ 'ìž‘ì€ë”°ì˜´í‘œ'ë¡œ ë³€í™˜ (í°ë”°ì˜´í‘œ ì¤‘ì²© ë°©ì§€ í•µì‹¬!)
        json_str = json_str.replace("ã€Œ", "'").replace("ã€", "'")
        json_str = json_str.replace("â€œ", "'").replace("â€", "'")
        json_str = json_str.replace("ã€Ž", "'").replace("ã€", "'")
        json_str = json_str.replace("â€˜", "'").replace("â€™", "'")
        
        # 2. ê°’ ë‚´ë¶€ì˜ ì¤„ë°”ê¿ˆ ì²˜ë¦¬
        def replace_br(match):
            # ë§¤ì¹­ëœ ê°’ ë‚´ë¶€ì˜ ì—”í„°ë§Œ \n ë¬¸ìžë¡œ ë°”ê¿ˆ
            return match.group(0).replace('\n', '\\n').replace('\r', '\\n')
        
        json_str = re.sub(r'":\s*"(.*?)"', replace_br, json_str, flags=re.DOTALL)

        # 3. íŒŒì´ì¬ ìŠ¤íƒ€ì¼ ë¶ˆë¦¬ì–¸/None ë³€í™˜
        json_str = json_str.replace('True', 'true').replace('False', 'false').replace('None', 'null')
        
        # 4. ì œì–´ ë¬¸ìž ì œê±° (ë¹„ì •ìƒì ì¸ ì•„ìŠ¤í‚¤ ë¬¸ìž ì œê±°)
        json_str = re.sub(r"[\x00-\x1F\x7F]", "", json_str)
        
        # 5. ë¶ˆí•„ìš”í•œ ê³µë°± ì •ë¦¬ (ì„ íƒì‚¬í•­)
        json_str = json_str.strip()
        
        return json_str


    # íšŒì‚¬ëª… ê³ ì •í•´ì•¼ í•  í•„ìš” O (ì•„ì§ ìˆ˜ì • ì „ìž„)
    def _extract_company_from_metadata(self, md: Dict) -> str:
        # ë©”íƒ€ë°ì´í„°ì—ì„œ íšŒì‚¬ëª…ì„ ì¶”ì¶œí•˜ëŠ” ë¡œì§
        source = md.get("source_file", "")
        if "í˜„ëŒ€" in source: return "í˜„ëŒ€í•´ìƒ"
        if "ì‚¼ì„±" in source: return "ì‚¼ì„±í™”ìž¬"
        if "DB" in source or "ë™ë¶€" in source: return "DBì†í•´ë³´í—˜"
        return md.get("company", "ì•Œ ìˆ˜ ì—†ìŒ")

    
    def _get_sum_insured(self, company: str, product: str) -> int:
        """
        ë³´í—˜ì‚¬ + ìƒí’ˆëª…ìœ¼ë¡œ ê°€ê²© ì¡°íšŒ (ìœ ì‚¬ë„ ë§¤ì¹­ í¬í•¨)
        """
        if not SUM_INSURED_MAP:
            return 10000  # ê¸°ë³¸ê°’
        
        if not company or not product:
            return 10000
        
        # ì •í™• ë§¤ì¹­
        if company in SUM_INSURED_MAP:
            if product in SUM_INSURED_MAP[company]:
                return SUM_INSURED_MAP[company][product]
            
            # ìœ ì‚¬ë„ ë§¤ì¹­
            import difflib
            products = list(SUM_INSURED_MAP[company].keys())
            matches = difflib.get_close_matches(product, products, n=1, cutoff=0.8)
            
            if matches:
                matched_product = matches[0]
                print(f"ðŸ” ê°€ê²© ë§¤ì¹­: '{product}' â†’ '{matched_product}'")
                return PRICE_MAP[company][matched_product]
        
        return 10000  # ê¸°ë³¸ê°’

    def _get_insurance_price(self, company: str, product: str) -> int:
        """
        ë³´í—˜ì‚¬ + ìƒí’ˆëª…ìœ¼ë¡œ ê°€ê²© ì¡°íšŒ (ìœ ì‚¬ë„ ë§¤ì¹­ í¬í•¨)
        """
        if not PRICE_MAP:
            return 10000  # ê¸°ë³¸ê°’
        
        if not company or not product:
            return 10000
        
        # ì •í™• ë§¤ì¹­
        if company in PRICE_MAP:
            if product in PRICE_MAP[company]:
                return PRICE_MAP[company][product]
            
            # ìœ ì‚¬ë„ ë§¤ì¹­
            import difflib
            products = list(PRICE_MAP[company].keys())
            matches = difflib.get_close_matches(product, products, n=1, cutoff=0.8)
            
            if matches:
                matched_product = matches[0]
                print(f"ðŸ” ê°€ê²© ë§¤ì¹­: '{product}' â†’ '{matched_product}'")
                return PRICE_MAP[company][matched_product]
        
        return 10000  # ê¸°ë³¸ê°’

    # RAGAS ë‹µë³€ í‰ê°€
    def _evaluate_response_quality(self, llm_response: str, analysis: Dict[str, Any]) -> float:
        score = 0.0
        if str(analysis.get("gestational_week", "")) in llm_response: score += 0.3
        if any(risk in llm_response for risk in analysis.get("risk_factors", [])): score += 0.4
        if "recommendations" in llm_response: score += 0.3
        return score

    def _log_rag_performance(self, rag_result: Dict, analysis: Dict, documents: List[Document]):
        try:
            log_data = {
                "timestamp": datetime.now().isoformat(),
                "user_profile": analysis,
                "response_quality_score": self._evaluate_response_quality(rag_result.get('answer', ''), analysis)
            }
            os.makedirs("../logs", exist_ok=True)
            with open("../logs/rag_performance.jsonl", 'a', encoding='utf-8') as f:
                f.write(json.dumps(log_data, ensure_ascii=False) + '\n')
        except: pass

    def _fallback_recommendation(self, user_profile: Dict[str, Any], health_status: Dict[str, Any]) -> Dict[str, Any]:
        return {"resultId": "fallback", "items": [], "rag_metadata": {"fallback": True}}

    def _analyze_user_profile(self, user_profile: Dict[str, Any], health_status: Dict[str, Any]) -> Dict[str, Any]:
        pregnancy_info = user_profile.get("pregnancyInfo") or user_profile.get("pregnancy_info") or {}

        gest_week = (
            user_profile.get("gestationalWeek")
            or user_profile.get("gestational_week")
            or pregnancy_info.get("gestationalWeek")
            or pregnancy_info.get("gestational_week")
            or 0
        )

        is_firstbirth = (
            user_profile.get("isFirstbirth")
            if "isFirstbirth" in user_profile
            else user_profile.get("is_firstbirth", pregnancy_info.get("isFirstbirth", pregnancy_info.get("is_firstbirth", True)))
        )

        is_multiple = (
            user_profile.get("isMultiplePregnancy")
            if "isMultiplePregnancy" in user_profile
            else user_profile.get("is_multiple_pregnancy", pregnancy_info.get("isMultiplePregnancy", pregnancy_info.get("is_multiple_pregnancy", False)))
        )

        miscarriage = (
            user_profile.get("miscarriageHistory")
            or user_profile.get("miscarriage_history")
            or pregnancy_info.get("miscarriageHistory")
            or pregnancy_info.get("miscarriage_history")
            or 0
        )

        analysis = {
            "gestational_week": int(gest_week) if str(gest_week).isdigit() else (gest_week or 0),
            "is_firstbirth": bool(is_firstbirth),
            "is_multiple_pregnancy": bool(is_multiple),
            "miscarriage_history": int(miscarriage) if str(miscarriage).isdigit() else (miscarriage or 0),
            "has_preeclampsia": False,
            "has_preterm_risk": False,
            "has_diabetes": False,
            "has_hypertension": False,
            "risk_factors": [],
        }

        past = health_status.get("pastDiseases") or health_status.get("past_diseases") or []
        chronic = health_status.get("chronicDiseases") or health_status.get("chronic_diseases") or []
        comps = health_status.get("pregnancyComplications") or health_status.get("pregnancy_complications") or []

        for d in past:
            if isinstance(d, dict) and (d.get("pastDiseaseType") or d.get("past_disease_type")) == "HYPERTENSION":
                analysis["has_hypertension"] = True
                analysis["risk_factors"].append("ê³ í˜ˆì••")

        for d in chronic:
            if isinstance(d, dict) and (d.get("chronicDiseaseType") or d.get("chronic_disease_type")) == "DIABETES":
                analysis["has_diabetes"] = True
                analysis["risk_factors"].append("ë‹¹ë‡¨")

        for c in comps:
            c_type = None
            if isinstance(c, str):
                c_type = c
            elif isinstance(c, dict):
                c_type = (
                    c.get("pregnancyComplicationType") 
                    or c.get("complication_type") 
                    or c.get("pregnancy_complication_type")
                )
            
            if c_type == "PREECLAMPSIA":
                analysis["has_preeclampsia"] = True
                analysis["risk_factors"].append("ìž„ì‹ ì¤‘ë…ì¦")
            elif c_type == "PRETERM_RISK":
                analysis["has_preterm_risk"] = True
                analysis["risk_factors"].append("ì¡°ì‚°ìœ„í—˜")

        return analysis

# ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° ì™¸ë¶€ í˜¸ì¶œ í•¨ìˆ˜
recommender = InsuranceRecommender()

def generate_recommendations_db(request) -> Dict[str, Any]:
    user_profile = request.user_profile.model_dump()
    health_status = request.health_status.model_dump()
    return recommender.generate_rag_recommendation(user_profile, health_status)