import json
import os
import uuid
import re
from typing import Dict, Any, List, Optional
from datetime import datetime

# FAISS ê¸°ë°˜ RAG + LLM ê´€ë ¨ ì„í¬íŠ¸
try:
    from langchain_community.vectorstores import FAISS
    from langchain_huggingface import HuggingFaceEmbeddings
    from langchain_core.documents import Document
    
    # rag_pipeline import (LLM + í”„ë¡¬í”„íŠ¸)
    from rag_pipeline import ask_question
    
    # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
    embeddings = HuggingFaceEmbeddings(
        model_name="jhgan/ko-sroberta-multitask",
        model_kwargs={"device": "cpu"},
        encode_kwargs={"normalize_embeddings": True},
    )
    
    # FAISS ë²¡í„°ìŠ¤í† ì–´ ì ˆëŒ€ ê²½ë¡œ ì„¤ì •
    CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
    faiss_path = os.path.join(CURRENT_DIR, "..", "faiss_index")
    
    # FAISS ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ
    if os.path.exists(os.path.join(faiss_path, "index.faiss")):
        vectorstore = FAISS.load_local(faiss_path, embeddings, allow_dangerous_deserialization=True)
        print(f"âœ… ê¸°ì¡´ FAISS ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œë¨: {vectorstore.index.ntotal}ê°œ ë¬¸ì„œ")
    else:
        vectorstore = None
        print("ğŸ†• FAISS ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì˜ˆì •")
    
    RAG_AVAILABLE = True
    LLM_AVAILABLE = True
    
except Exception as e:
    print(f"âŒ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    vectorstore = None
    embeddings = None
    RAG_AVAILABLE = False
    LLM_AVAILABLE = False

# ë³´í—˜ë£Œ ë° ê°€ì…ê¸ˆì•¡ í…Œì´ë¸” ë¡œë“œ
PRICE_MAP = {}
SUM_INSURED_MAP = {}
PRICE_FILE = os.path.join(CURRENT_DIR, "prices.json")
SUM_INSURED_FILE = os.path.join(CURRENT_DIR, "sum_insured.json")

def _load_data_maps():
    global PRICE_MAP, SUM_INSURED_MAP
    for file_path, target_map, name in [(PRICE_FILE, PRICE_MAP, "ë³´í—˜ë£Œ"), (SUM_INSURED_FILE, SUM_INSURED_MAP, "ê°€ì…ê¸ˆì•¡")]:
        if os.path.exists(file_path):
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    target_map.update(json.load(f))
                print(f"âœ… {name} í…Œì´ë¸” ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                print(f"âŒ {name} ë¡œë“œ ì‹¤íŒ¨: {e}")

_load_data_maps()

class InsuranceRecommender:
    def __init__(self):
        self.vectorstore = vectorstore
        self.embeddings = embeddings
        if RAG_AVAILABLE:
            self._load_insurance_data()

    def _load_insurance_data(self):
        """ì ˆëŒ€ ê²½ë¡œë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  JSON ë°ì´í„°ë¥¼ FAISSì— ë¡œë“œ"""
        try:
            # ì´ë¯¸ ë¡œë“œë˜ì—ˆë‹¤ë©´ ìŠ¤í‚µ
            if self.vectorstore and self.vectorstore.index.ntotal > 0:
                return

            documents = []
            data_dir = os.path.join(CURRENT_DIR, "..", "json", "Llama_json")
            
            if not os.path.exists(data_dir):
                print(f"âŒ ë°ì´í„° ë””ë ‰í† ë¦¬ ì—†ìŒ: {data_dir}")
                return

            json_files = [f for f in os.listdir(data_dir) if f.endswith('.json')]
            print(f"ğŸ“‚ {len(json_files)}ê°œ íŒŒì¼ ë¶„ì„ ì¤‘...")

            for filename in json_files:
                filepath = os.path.join(data_dir, filename)
                with open(filepath, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    items = data if isinstance(data, list) else [data]
                    for item in items:
                        content = item.get('content', '').strip()
                        if content and len(content) > 20:
                            doc = Document(
                                page_content=content,
                                metadata={**item.get('metadata', {}), 'source_file': filename}
                            )
                            documents.append(doc)
            
            if documents:
                self.vectorstore = FAISS.from_documents(documents, self.embeddings)
                os.makedirs(faiss_path, exist_ok=True)
                self.vectorstore.save_local(faiss_path)
                print(f"âœ… FAISS ìƒì„± ì™„ë£Œ: {len(documents)}ê°œ ë¬¸ì„œ")
        except Exception as e:
            print(f"âŒ ë°ì´í„° ë¡œë“œ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")

    def search_relevant_documents(self, query: str, n_results: int = 10) -> List[Document]:
        if not self.vectorstore:
            print("âš ï¸ ê²€ìƒ‰ ë¶ˆê°€: ë²¡í„°ìŠ¤í† ì–´ê°€ ë¹„ì–´ìˆìŒ")
            return []
        try:
            # ê²€ìƒ‰ ì„±ëŠ¥ì„ ìœ„í•´ scoreì™€ í•¨ê»˜ ê²€ìƒ‰
            docs_with_scores = self.vectorstore.similarity_search_with_score(query, k=n_results)
            return [doc for doc, score in docs_with_scores]
        except Exception as e:
            print(f"âŒ FAISS ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

    def generate_rag_recommendation(self, user_profile: Dict[str, Any], health_status: Dict[str, Any]) -> Dict[str, Any]:
        try:
            # 1. ì‚¬ìš©ì ë¶„ì„
            analysis = self._analyze_user_profile(user_profile, health_status)
            
            # 2. ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„± ë° ë¬¸ì„œ ê²€ìƒ‰
            search_query = self._build_rag_query(analysis)
            relevant_docs = self.search_relevant_documents(search_query, n_results=12)
            
            if not relevant_docs:
                print("âš ï¸ ê²€ìƒ‰ëœ ë¬¸ì„œ ì—†ìŒ -> Fallback ì‹¤í–‰")
                return self._fallback_recommendation(user_profile, health_status)
            
            # 3. LLM ì§ˆë¬¸ ìƒì„± ë° í˜¸ì¶œ
            context = self._build_context_from_documents(relevant_docs)
            llm_question = self._build_llm_question(analysis, context)
            
            print(f"ğŸ¤– LLM ìš”ì²­ ì¤‘... (ì£¼ìˆ˜: {analysis['gestational_week']}ì£¼)")
            rag_result = ask_question(llm_question, profile=analysis)
            
            if rag_result and 'answer' in rag_result:
                result = self._parse_llm_response_to_recommendation(rag_result['answer'], analysis, relevant_docs)
                # ë§Œì•½ íŒŒì‹± ê²°ê³¼ê°€ ë¹„ì–´ìˆë‹¤ë©´ fallback
                if not result.get("items"):
                    return self._fallback_recommendation(user_profile, health_status)
                return result
                
            return self._fallback_recommendation(user_profile, health_status)
        except Exception as e:
            print(f"âŒ RAG í”„ë¡œì„¸ìŠ¤ ì‹¤íŒ¨: {e}")
            return self._fallback_recommendation(user_profile, health_status)

    def _build_rag_query(self, analysis: Dict[str, Any]) -> str:
        # ê²€ìƒ‰ í™•ë¥ ì„ ë†’ì´ê¸° ìœ„í•´ í•µì‹¬ í‚¤ì›Œë“œ ìœ„ì£¼ë¡œ êµ¬ì„±
        parts = ["ì„ì‹  ë³´í—˜", "íƒœì•„ ë³´ì¥"]
        week = analysis.get('gestational_week', 0)
        if week > 0: parts.append(f"{week}ì£¼")
        if analysis.get("is_multiple_pregnancy"): parts.append("ë‹¤íƒœì•„ ìŒë‘¥ì´")
        if analysis.get("risk_factors"): parts.extend(analysis.get("risk_factors")[:2])
        return " ".join(parts)

    def _build_context_from_documents(self, documents: List[Document]) -> str:
        parts = []
        for i, doc in enumerate(documents[:8]): # ì»¨í…ìŠ¤íŠ¸ ìµœì í™”ë¥¼ ìœ„í•´ 8ê°œë¡œ ì œí•œ
            md = doc.metadata or {}
            parts.append(f"[ë¬¸ì„œ {i+1}] ìƒí’ˆ:{md.get('product_name','?')}, í˜ì´ì§€:{md.get('page_number','?')}\në‚´ìš©:{doc.page_content[:800]}")
        return "\n\n".join(parts)

    def _build_llm_question(self, analysis: Dict[str, Any], context: str) -> str:
        return f"""
ì—­í• : ë³´í—˜ ì „ë¬¸ ì–¸ë”ë¼ì´í„°
ì„ì‹ ë¶€ ì •ë³´: {analysis['gestational_week']}ì£¼ì°¨, ìœ„í—˜ìš”ì¸({analysis.get('risk_factors', [])}), ë‹¤íƒœì•„({analysis['is_multiple_pregnancy']})

ì§€ì¹¨:
1. ì œê³µëœ [ë³´í—˜ ì•½ê´€ ì •ë³´]ë§Œ ê·¼ê±°ë¡œ ê°€ì¥ ì í•©í•œ ë³´í—˜ ìƒí’ˆ 2-3ê°œë¥¼ ì¶”ì²œí•˜ë¼.
2. ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•˜ë¼.
3. 'evidence'ëŠ” ë¬¸ë§¥ì—ì„œ ê·¸ëŒ€ë¡œ ì¸ìš©í•œ ë¬¸ì¥ê³¼ í˜ì´ì§€ë¥¼ í¬í•¨í•˜ë¼.

[ë³´í—˜ ì•½ê´€ ì •ë³´]
{context}

ì¶œë ¥ í˜•ì‹:
{{
  "recommendations": [
    {{
      "company": "ë³´í—˜ì‚¬ëª…",
      "product": "ìƒí’ˆëª…",
      "monthly_cost": 30000,
      "reason": "ì£¼ìˆ˜ì™€ ìœ„í—˜ìš”ì¸ì„ ê³ ë ¤í•œ êµ¬ì²´ì  ì¶”ì²œ ì´ìœ ",
      "special_contracts": ["ì¶”ì²œíŠ¹ì•½1", "ì¶”ì²œíŠ¹ì•½2"],
      "evidence": "ì¸ìš©ë¬¸... (page=ìˆ«ì)"
    }}
  ]
}}
"""

    def _parse_llm_response_to_recommendation(self, llm_response: str, analysis: Dict[str, Any], relevant_docs: List[Document]) -> Dict[str, Any]:
        try:
            json_block = re.search(r"(\{.*\})", llm_response, re.DOTALL)
            if not json_block: return {"items": []}
            
            data = json.loads(self._fix_json_string(json_block.group(1)))
            recs = data.get("recommendations", [])
            
            items = []
            for idx, rec in enumerate(recs[:3]):
                doc = relevant_docs[idx] if idx < len(relevant_docs) else relevant_docs[0]
                md = doc.metadata or {}
                
                # ê°€ì…ê¸ˆì•¡ ë° ë³´í—˜ë£Œ í…Œì´ë¸” ë§¤ì¹­
                comp = rec.get("company", "ì•Œ ìˆ˜ ì—†ìŒ")
                prod = rec.get("product", "ì•Œ ìˆ˜ ì—†ìŒ")
                
                items.append({
                    "itemId": uuid.uuid4().hex[:8],
                    "insurance_company": comp,
                    "product_name": prod,
                    "is_long_term": True,
                    "sum_insured": self._get_sum_insured(comp, prod),
                    "monthly_cost": str(self._get_insurance_price(comp, prod)),
                    "insurance_recommendation_reason": rec.get("reason", ""),
                    "special_contracts": [
                        {
                            "contract_name": str(c),
                            "contract_description": "ì•½ê´€ ê¸°ë°˜ ë§ì¶¤ ë³´ì¥",
                            "contract_recommendation_reason": f"{analysis['gestational_week']}ì£¼ì°¨ ë§ì¶¤ íŠ¹ì•½",
                            "key_features": ["ë³´ì¥ ë²”ìœ„ í™•ì¸ ì™„ë£Œ"],
                            "page_number": int(md.get("page_number", 1))
                        } for c in rec.get("special_contracts", [])
                    ],
                    "evidence_sources": [{"page_number": int(md.get("page_number", 1)), "text_snippet": rec.get("evidence", "")}]
                })
            
            return {
                "resultId": uuid.uuid4().hex[:8],
                "items": items,
                "rag_metadata": {"documents_used": len(relevant_docs), "gestational_week": analysis['gestational_week']}
            }
        except:
            return {"items": []}

    def _fix_json_string(self, s: str) -> str:
        s = s.replace("ã€Œ", "'").replace("ã€", "'").replace("â€œ", "'").replace("â€", "'")
        return s.replace('True', 'true').replace('False', 'false').replace('None', 'null')

    def _get_sum_insured(self, c, p): return SUM_INSURED_MAP.get(c, {}).get(p, 10000000)
    def _get_insurance_price(self, c, p): return PRICE_MAP.get(c, {}).get(p, 30000)

    def _fallback_recommendation(self, up, hs):
        return {"resultId": "fallback", "items": [], "rag_metadata": {"fallback": True}}

    def _analyze_user_profile(self, user_profile: Dict[str, Any], health_status: Dict[str, Any]) -> Dict[str, Any]:
        """Javaì˜ ë‹¤ì–‘í•œ í•„ë“œëª…(Camel/Snake) ì™„ë²½ ëŒ€ì‘"""
        
        # 1. ì„ì‹  ì •ë³´ ì¶”ì¶œ (ì¤‘ì²© ê°ì²´ ë˜ëŠ” ìµœìƒìœ„ í•„ë“œ)
        p_info = user_profile.get("pregnancyInfo") or user_profile
        
        gest_week = p_info.get("gestationalWeek") or p_info.get("gestational_week") or 0
        is_multiple = p_info.get("isMultiplePregnancy") or p_info.get("is_multiple_pregnancy") or False
        miscarriage = p_info.get("miscarriageHistory") or p_info.get("miscarriage_history") or 0

        analysis = {
            "gestational_week": int(gest_week),
            "is_multiple_pregnancy": bool(is_multiple),
            "miscarriage_history": int(miscarriage),
            "risk_factors": []
        }

        # 2. ê±´ê°• ìƒíƒœ ë¶„ì„ (í•©ë³‘ì¦ ìœ„ì£¼)
        comps = health_status.get("pregnancyComplications") or health_status.get("pregnancy_complications") or []
        for c in comps:
            c_type = c if isinstance(c, str) else (c.get("pregnancyComplicationType") or c.get("complication_type"))
            if c_type == "PREECLAMPSIA": analysis["risk_factors"].append("ì„ì‹ ì¤‘ë…ì¦")
            elif c_type == "PRETERM_RISK": analysis["risk_factors"].append("ì¡°ì‚°ìœ„í—˜")
        
        return analysis

recommender = InsuranceRecommender()