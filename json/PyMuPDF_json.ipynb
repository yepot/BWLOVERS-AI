{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb0f3ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: data/ë¬´ë°°ë‹¹ í˜„ëŒ€í•´ìƒ êµ¿ì•¤êµ¿ì–´ë¦°ì´ì¢…í•©ë³´í—˜Q(Hi2509).pdf\n",
      "ëª¨ë“  PDFì˜ JSON ë³€í™˜ ë° ê°œë³„ ì €ì¥ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import fitz \n",
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "def extract_insurance_to_json(pdf_path):\n",
    "    if not os.path.exists(pdf_path):\n",
    "        print(f\"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdf_path}\")\n",
    "        return\n",
    "\n",
    "    doc = fitz.open(pdf_path)\n",
    "    file_name = os.path.basename(pdf_path)\n",
    "    # íŒŒì¼ëª…ì—ì„œ .pdf ì œê±°í•˜ì—¬ ìƒí’ˆëª… ì¶”ì¶œ\n",
    "    product_name = file_name.replace(\".pdf\", \"\")\n",
    "    \n",
    "    structured_list = []\n",
    "    \n",
    "    # ìƒíƒœ ì¶”ì  ë³€ìˆ˜\n",
    "    current_clause_type = \"ê³µí†µ/ê°€ì´ë“œ\" \n",
    "    current_section = \"ëª©ì°¨ ë° ì•ˆë‚´\"\n",
    "\n",
    "    # í‘œì¤€ ì¹´í…Œê³ ë¦¬ ë§µ\n",
    "    category_map = {\n",
    "        \"ë¯¼ì›ì‚¬ë¡€\": [\"ìì£¼ ë°œìƒí•˜ëŠ” ë¯¼ì›\", \"ë¯¼ì› ì˜ˆì‹œ\", \"ë¶„ìŸ ì‚¬ë¡€\", \"Q&A\", \"ì£¼ìš” ë¯¼ì›\", \"ë¯¼ì›ì‚¬ë¡€\"],\n",
    "        \"ìœ ì˜ì‚¬í•­\": [\"ìœ ì˜ì‚¬í•­\", \"ë°˜ë“œì‹œ ì•Œì•„ë‘ì–´ì•¼ í• \", \"ì£¼ì˜ì‚¬í•­\", \"ì¤‘ìš”ì‚¬í•­\", \"í•µì‹¬ì²´í¬\", \"ê°€ì…ì ìœ ì˜ì‚¬í•­\"],\n",
    "        \"ë©´ì±…ì‚¬í•­\": [\"ì§€ê¸‰í•˜ì§€ ì•ŠëŠ” ì‚¬ìœ \", \"ë³´ìƒí•˜ì§€ ì•ŠëŠ” ì‚¬í•­\", \"ë³´í—˜ê¸ˆì„ ì§€ê¸‰í•˜ì§€ ì•ŠëŠ”\", \"ë©´ì±…\"],\n",
    "        \"ì§€ê¸‰ì‚¬ìœ \": [\"ë³´í—˜ê¸ˆì˜ ì§€ê¸‰ì‚¬ìœ \", \"ì§€ê¸‰í•˜ëŠ” ì‚¬ìœ \", \"ë³´ì¥ë‚´ìš©\", \"ë³´í—˜ê¸ˆ ì§€ê¸‰ ì‚¬ìœ \"],\n",
    "        \"ìš©ì–´ì •ì˜\": [\"ìš©ì–´ì˜ ì •ì˜\", \"ìš©ì–´í•´ì„¤\", \"ìš©ì–´ì •ì˜\"]\n",
    "    }\n",
    "\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        \n",
    "        # 2ì—´ êµ¬ì¡° í•´ê²°: ì™¼ìª½/ì˜¤ë¥¸ìª½ ë¶„í•  ì¶”ì¶œ\n",
    "        w = page.rect.width\n",
    "        h = page.rect.height\n",
    "        left_text = page.get_text(\"text\", clip=fitz.Rect(0, 0, w / 2, h))\n",
    "        right_text = page.get_text(\"text\", clip=fitz.Rect(w / 2, 0, w, h))\n",
    "        full_text = left_text + \"\\n\" + right_text\n",
    "\n",
    "        # ìƒë‹¨ í…ìŠ¤íŠ¸ë¡œ ë©”íƒ€ë°ì´í„° ê°ì§€\n",
    "        header_text = full_text[:1200]\n",
    "\n",
    "        # ë³´í†µ/íŠ¹ë³„ì•½ê´€ ìë™ ê°ì§€\n",
    "        if \"íŠ¹ë³„ì•½ê´€\" in header_text or \"íŠ¹ì•½\" in header_text:\n",
    "            current_clause_type = \"íŠ¹ë³„ì•½ê´€\"\n",
    "        elif \"ë³´í†µì•½ê´€\" in header_text:\n",
    "            current_clause_type = \"ë³´í†µì•½ê´€\"\n",
    "\n",
    "        # ì„¹ì…˜ íƒ€ì´í‹€ ê°ì§€ (ì œXê´€, ì œXì¥)\n",
    "        section_match = re.search(r\"(ì œ\\s?\\d+\\s?[ê´€ì¥]\\s+[^#\\n\\r]+)\", header_text)\n",
    "        if section_match:\n",
    "            current_section = section_match.group(1).strip()\n",
    "\n",
    "        # ì¹´í…Œê³ ë¦¬ ê°ì§€\n",
    "        detected_category = \"ì¼ë°˜ì¡°í•­\"\n",
    "        for cat, keywords in category_map.items():\n",
    "            if any(kw in full_text for kw in keywords):\n",
    "                detected_category = cat\n",
    "                break\n",
    "\n",
    "        # í˜ì´ì§€ ë‹¨ìœ„ ë°ì´í„° ìƒì„±\n",
    "        structured_list.append({\n",
    "            \"content\": full_text.strip(),\n",
    "            \"metadata\": {\n",
    "                \"product_name\": product_name,\n",
    "                \"page_number\": page_num + 1,\n",
    "                \"clause_type\": current_clause_type,\n",
    "                \"section_title\": current_section,\n",
    "                \"category\": detected_category,\n",
    "                \"is_special_content\": detected_category in [\"ë¯¼ì›ì‚¬ë¡€\", \"ìœ ì˜ì‚¬í•­\"],\n",
    "                \"source_file\": file_name\n",
    "            }\n",
    "        })\n",
    "\n",
    "    # .json íŒŒì¼ ì €ì¥\n",
    "    output_filename = f\"structured_{product_name}.json\"\n",
    "    with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(structured_list, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    doc.close()\n",
    "    print(f\"ì €ì¥ ì™„ë£Œ: {output_filename}\")\n",
    "\n",
    "pdf_list = [\n",
    "    \"data/ë¬´ë°°ë‹¹ í˜„ëŒ€í•´ìƒ êµ¿ì•¤êµ¿ì–´ë¦°ì´ì¢…í•©ë³´í—˜Q(Hi2509).pdf\"\n",
    "]\n",
    "\n",
    "for pdf_path in pdf_list:\n",
    "    extract_insurance_to_json(pdf_path)\n",
    "\n",
    "print(\"ëª¨ë“  PDFì˜ JSON ë³€í™˜ ë° ê°œë³„ ì €ì¥ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da2926f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from google.colab import userdata\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Ragas ë° LangChain í•„ìˆ˜ ì„í¬íŠ¸\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "from langchain_classic.chains import create_retrieval_chain\n",
    "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"jhgan/ko-sroberta-multitask\",\n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "router_llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# JSON íŒŒì¼ ë¡œë“œ ë° ë©”ëª¨ë¦¬ Vector DB ìƒì„±\n",
    "JSON_FILE_PATH = 'json/PyMuPDF_json/structured_KB ë‹¤ì´ë ‰íŠ¸ ìë…€ë³´í—˜(ë¬´ë°°ë‹¹)ì•½ê´€.json'\n",
    "\n",
    "with open(JSON_FILE_PATH, 'r', encoding='utf-8') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "# JSON ë°ì´í„°ë¥¼ LangChain Document ê°ì²´ë¡œ ë³€í™˜\n",
    "documents = []\n",
    "for item in json_data:\n",
    "    content = item.get(\"content\", \"\")\n",
    "    meta = item.get(\"metadata\", {})\n",
    "    \n",
    "    new_doc = Document(\n",
    "        page_content=content,\n",
    "        metadata={\n",
    "            \"source\": meta.get(\"source_file\", \"KB_Insurance\"),\n",
    "            \"page\": meta.get(\"page_number\", \"ì •ë³´ ì—†ìŒ\")\n",
    "        }\n",
    "    )\n",
    "    documents.append(new_doc)\n",
    "\n",
    "# persist_directoryë¥¼ ì§€ì •í•˜ì§€ ì•Šì•„ ë©”ëª¨ë¦¬ ìƒì—ë§Œ ì¡´ì¬í•˜ëŠ” ì„ì‹œ DB ìƒì„±\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embedding_model,\n",
    "    collection_name=\"kb_child_only\"\n",
    ")\n",
    "\n",
    "# ê²€ìƒ‰ê¸° ì •ì˜ (ìƒìœ„ 5ê°œ ì¡°í•­ ì¶”ì¶œ)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "print(f\"âœ… {len(documents)}ê°œì˜ KB ì•½ê´€ ì¡°í•­ì´ ë©”ëª¨ë¦¬ DBì— ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ ì •ì˜\n",
    "base_system = \"\"\"\n",
    "[ì—­í• ] ë„ˆëŠ” ë³´í—˜ ì•½ê´€ ì „ë¬¸ ë¶„ì„ê°€ë‹¤. ì œê³µëœ ë¬¸ë§¥(context)ë§Œ ê·¼ê±°ë¡œ ë‹µí•˜ë¼.\n",
    "[ê·œì¹™]\n",
    "1) {context}ì— ì—†ëŠ” ë‚´ìš©ì€ 'ì •ë³´ ì—†ìŒ'ìœ¼ë¡œ ë‹µí•  ê²ƒ.\n",
    "2) ë‹µë³€ì—ëŠ” ë°˜ë“œì‹œ í•´ë‹¹ ì¡°í•­ì˜ í˜ì´ì§€(metadataì˜ page)ë¥¼ í¬í•¨í•  ê²ƒ.\n",
    "3) ë¶ˆí•„ìš”í•œ ì¤‘ë³µ ë³´ì¥ì„ í”¼í•  ìˆ˜ ìˆëŠ” ëŒ€ì•ˆì„ ì œì‹œí•  ê²ƒ.\n",
    "\"\"\"\n",
    "\n",
    "summary_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", base_system + \"\\n\\n[ì§ˆë¬¸ ìœ í˜•: ìš”ì•½/ê²€ìƒ‰]\\nì œê³µëœ ë¬¸ë§¥:\\n{context}\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "recommend_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", base_system + \"\\n\\n[ì§ˆë¬¸ ìœ í˜•: ì¶”ì²œ]\\nì œê³µëœ ë¬¸ë§¥:\\n{context}\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# RAG ì²´ì¸ êµ¬ì¶•\n",
    "summary_chain = create_retrieval_chain(\n",
    "    retriever,\n",
    "    create_stuff_documents_chain(llm, summary_prompt)\n",
    ")\n",
    "recommend_chain = create_retrieval_chain(\n",
    "    retriever,\n",
    "    create_stuff_documents_chain(llm, recommend_prompt)\n",
    ")\n",
    "\n",
    "def classify_query(query: str) -> str:\n",
    "    # ê°„ë‹¨í•œ ë¼ìš°íŒ… ë¡œì§\n",
    "    if \"ì¶”ì²œ\" in query or \"ë§ëŠ”\" in query:\n",
    "        return \"recommend\"\n",
    "    return \"summary_search\"\n",
    "\n",
    "def ask_question_for_ragas(query: str):\n",
    "    qtype = classify_query(query)\n",
    "    if qtype == \"recommend\":\n",
    "        response = recommend_chain.invoke({\"input\": query})\n",
    "    else:\n",
    "        response = summary_chain.invoke({\"input\": query})\n",
    "    \n",
    "    return {\n",
    "        \"question\": query,\n",
    "        \"answer\": response[\"answer\"],\n",
    "        \"contexts\": [doc.page_content for doc in response[\"context\"]]\n",
    "    }\n",
    "\n",
    "# ì§ˆë¬¸ ì‹¤í–‰ ë° Ragas í‰ê°€\n",
    "test_questions = [\n",
    "    \"ì„ì‹  10ì£¼ì°¨ ì‚°ëª¨ë¥¼ ìœ„í•œ ì¶”ì²œ íŠ¹ì•½ê³¼ ê·¸ ì´ìœ ë¥¼ ì•½ê´€ ê·¼ê±°(í˜ì´ì§€ í¬í•¨)ë¥¼ ë“¤ì–´ ì„¤ëª…í•´ì¤˜.\",\n",
    "    \"ì´ ë³´í—˜ ì•½ê´€ì—ì„œ ì•„ì´ê°€ ì…ì›í–ˆì„ ë•Œ ë³´ì¥ë°›ì„ ìˆ˜ ìˆëŠ” ì¡°í•­ë“¤ì„ ìš”ì•½í•´ì¤˜.\",\n",
    "    \"ì•„ë™ì˜ ì•” ì§„ë‹¨ê³¼ ê´€ë ¨ëœ ì¡°í•­ì„ ì°¾ì•„ì¤˜.\"\n",
    "]\n",
    "\n",
    "rag_data = []\n",
    "for q in test_questions:\n",
    "    print(f\"ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘: {q[:30]}...\")\n",
    "    rag_data.append(ask_question_for_ragas(q))\n",
    "\n",
    "# Ragas ë°ì´í„°ì…‹ ë³€í™˜ ë° í‰ê°€\n",
    "df = pd.DataFrame(rag_data)\n",
    "ragas_dataset = Dataset.from_pandas(df)\n",
    "\n",
    "results = evaluate(\n",
    "    dataset=ragas_dataset,\n",
    "    metrics=[faithfulness, answer_relevancy],\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(\"\\nğŸ“Š Ragas í‰ê°€ ê²°ê³¼ (KB ì•½ê´€ ì „ìš©)\")\n",
    "print(results.to_pandas()[['faithfulness', 'answer_relevancy']].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636ba5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = results.to_pandas()\n",
    "\n",
    "q_col = 'question' if 'question' in df_results.columns else 'user_input'\n",
    "a_col = 'answer' if 'answer' in df_results.columns else 'response'\n",
    "c_col = 'contexts' if 'contexts' in df_results.columns else 'retrieved_contexts'\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ëª¨ë“  ì§ˆë¬¸ì— ëŒ€í•œ RAG ë‹µë³€ ë° í‰ê°€ ê²°ê³¼\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for i, row in df_results.iterrows():\n",
    "    print(f\"\\n[ì§ˆë¬¸ {i+1}]\")\n",
    "    print(f\"â“ {row[q_col]}\")  \n",
    "    print(\"-\" * 30)\n",
    "    print(f\"[LLMì˜ ë‹µë³€]\\n{row[a_col]}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    f_score = row.get('faithfulness', 0)\n",
    "    r_score = row.get('answer_relevancy', 0)\n",
    "    print(f\"ğŸ“Š í‰ê°€ ì ìˆ˜: ì¶©ì‹¤ë„ {f_score:.2f} | ê´€ë ¨ì„± {r_score:.2f}\")\n",
    "    \n",
    "    print(f\"\\n[ì°¸ì¡° ë¬¸ë§¥ ì¼ë¶€]\")\n",
    "    for j, ctx in enumerate(row[c_col]):\n",
    "      print(f\"ë¬¸ë§¥ {j+1}: {ctx[:150]}...\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
